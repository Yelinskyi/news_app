[
  {
    "title": "OpenAI to use GPT-4 LLM for content moderation, warns against bias",
    "date": "8/17/2023, 1:52:00 PM",
    "text": "The company expects to eliminate undesired biases introduced during training with the involvement of humans in the loop.\n\nChatGPT-creator OpenAI is working on the development of its GPT-4 large language model (LLM) to automate the process of content moderation across digital platforms, especially social media.\n\nOpenAI is exploring the use of GPT-4’s ability to interpret rules and nuances in long content policy documentation, along with its capability to adapt instantly to policy updates, the company said in a blog post.\n\n\"We believe this offers a more positive vision of the future of digital platforms, where AI can help moderate online traffic according to platform-specific policy and relieve the mental burden of a large number of human moderators,\" the company said, adding that anyone with access to OpenAI’s API can implement their own moderation system.\n\nIn contrast to the present practice of content moderation, which is completely manual and time consuming, OpenAI’s GPT-4 large language model can be used to create custom content policies in hours, the company said.\n\nIn order to do so, data scientists and engineers can use a policy guideline crafted by policy experts and data sets containing real-life examples of such policy violations in order to label the data.\n\nHumans to help test AI content moderation\n\n\"Then, GPT-4 reads the policy and assigns labels to the same dataset, without seeing the answers. By examining the discrepancies between GPT-4's judgments and those of a human, the policy experts can ask GPT-4 to come up with reasoning behind its labels, analyze the ambiguity in policy definitions, resolve confusion and provide further clarification in the policy accordingly,\" the company said.\n\nThese steps may be repeated by data scientists and engineers before the large language model can generate satisfying results, it added, explaining that the iterative process yields refined content policies that are translated into classifiers, enabling the deployment of the policy and content moderation at scale.\n\nOther advantages of using GPT-4 over the present manual approach to content moderation include a decrease in inconsistent labelling and faster feedback loop, according to OpenAI.\n\n\"People may interpret policies differently or some moderators may take longer to digest new policy changes, leading to inconsistent labels. In comparison, LLMs are sensitive to granular differences in wording and can instantly adapt to policy updates to offer a consistent content experience for users,\" the company said.\n\nThe new approach, according to the company, also takes less effort in terms of training the model.\n\nFurther, OpenAI claims that this approach is different from the so-called constitutional AI, under which content moderation is dependent on the model's own internalized judgment of what is safe. Various companies, including Anthropic, have taken a constitutional AI approach in training their models to be free of bias and error.\n\nNevertheless, OpenAI warned that undesired biases may creep into content moderation models during training.\n\n\"As with any AI application, results and output will need to be carefully monitored, validated, and refined by maintaining humans in the loop,\" it said.\n\nSome industry experts think OpenAI's approach to content moderation has potential. \"GPT-4 is a super capable model and OpenAI has a never-ending stream of users trying to make it do harmful things. Which is great training data,\" said Tobias Zwingmann, managing partner at AI services company Rapyd.AI.\n\nWhether AI can handle all content moderation tasks, though, is an open question.\n\n\"The real question is how much automation of content moderation makes sense. There would seem to be approaches where some automation would, perhaps in sifting content, identifying targets and making recommendations,\" said Mark Beccue, AI research director at The Futurum Group.\n\nMeanwhile, the lack of any sort of announcement or guidance from Facebook regarding AI's use for content moderation casts some doubt on the technology's potential efficacy for that particular application, since Facebook is also a generative AI leader, Beccue noted.\n\n\"Meta is one of the world leaders in AI innovation,\" he said. \"Wouldn't you think Meta would be very focused on developing content moderation automation? Doesn't the lack of what Open AI is proposing from Meta say something about that? IOW, why would Open AI know something Meta doesn't know about content moderation?  Bottom line — GPT-4 as content moderation is perhaps an experiment worth sandboxing, but no guarantees.\"\n\nIf OpenAI’s large language model can be used successfully for content moderation, it will open up a multibillion-dollar market for the company.\n\nThe global content moderation services market, according to a report from Allied Market Research, was valued at $8.5 billion in 2021, and is projected to reach $26.3 billion by 2031, growing at a compound annual growth rate of 12.2% from 2022 to 2031.",
    "comments": [
      { "author": "Bob", "date": "8/17/2023, 1:52:00 PM", "text": "Chat GPT really heps me." },
      { "author": "Sam", "date": "8/17/2023, 2:37:00 PM", "text": "Great news." }
    ]
  },
  {
    "title": "Jamf Threat Labs subverts iPhone security with fake Airplane Mode",
    "date": "8/17/2023, 8:09:00 AM",
    "text": "Researchers built a proof-of-concept attack that tricks victims into thinking they are using Airplane Mode while attackers continue to gain network access.\n\nFresh security research from Jamf Threat Labs may not reflect an active attack, but it does illustrate the layered complexity of today’s threat environment.\n\nWhen Airplane mode isn’t Airplane mode\nIn brief, the researchers have figured out a proof of concept attack that tricks victims into thinking they are using Airplane Mode. However, in reality the attacker has put in place a fake version of that mode that looks normal but lets the attacker maintain access to the device.\n\nThis is by no means a straightforward attack and hasn’t been seen in the wild. The exploit is complex and would require an attacker to successfully take control of the target device through a series of exploits, the research claims.\n\nThis is a crafty attack in that internet connections to a user’s apps are cut when they enter the fake Airplane Mode, so it feels like it is working. And yet, all the while, the exploit can continue to exfiltrate data from the device. The researchers point out that some iPhone users switch to Airplane Mode to improve their own security.\n\nIt's not a trivial hack\nIn order to put the exploit in place, the team replaced elements at an OS level, including altering hard-to-find commands within the operating system.\n\nTo make it convincing, they also had to figure out how to make the user interface act as if things were offline when in the fake mode and also how to fool the OS into cutting off network access to everything except the malware inside the device.\n\nShould we worry?\nApple maintains the world’s most secure platforms, but doing so is a constant campaign. While major security incidents are relatively infrequent, they do exist. Indeed, while it’s true to observe that an industry of security experts dedicated to identifying threats in order to sell people protection against such abuses does exist, that doesn’t mean it should not exist.\n\nThink about Apple’s own actions. Its decision to introduce Lockdown Mode was a direct reflection of the increasingly complex threat environments in which we exist. Introduction of that mode followed highly publicized attacks by NSO Group and others.\n\nAnd more recently, AT&T Alien Labs researchers claim to have identified around 10,000 infected Macs that are being used to support AdLoad malware, that report suggests.\n\n“Users of MacOS devices are a lucrative target for the adversaries behind this malware and are being tricked to download and install unwanted applications,” that report claimed.\n\nReality denial\nResearch and proof of concept cases like these don’t suggest Apple’s platforms are becoming more insecure but should be seen as warnings that attempts to undermine platform security are intensifying.\n\nThat doesn’t mean every Mac, iPad, or iPhone user — or every fleet manager — must immediately switch off all the devices, limit network access, and invest in every kind of malware protection tech available to us here on our still green in patches planet.\n\nWhat it does mean is that attempts to secure the most vulnerable point in any tech — the humans using it — must be prioritized.\n\nAfter all, as the Jamf proof of concept shows, what you think you see may not always be what is there. That is also, of course, why it’s somewhat inevitable that on-device telematic security monitoring will form part of the future of Apple platform security. Which is probably why Jamf acquired ZecOps in 2022.",
    "comments": [
      { "author": "Jim", "date": "8/17/2023, 8:37:00 AM", "text": "Interesting..." },
      { "author": "Kelly", "date": "8/17/2023, 9:11:00 AM", "text": "I like it." }
    ]
  },
  {
    "title": "Opera browser weaves Aria AI into iOS app",
    "date": "8/16/2023, 11:00:00 AM",
    "text": "Multiplatform web browser Opera has its own AI assistant, Aria, now offered for iOS and available on all major platforms.\n\nOpera, maker of the well-known browser of the same name, said today that it has successfully integrated its AI assistant, Aria, into the iOS version of the browser, making it available for the first time on all major platforms.\n\nOpera said that it has seen rapid growth in the use of Aria, which now boasts one million users. The AI assistant, like many in use now, is based on a collaboration with OpenAI, giving users access to that company’s large language models without creating a separate user account. An Opera account is required to use Aria, however. Aria also uses Opera’s in-house Composer architecture for access to real-time web results, grafting a layer of live information onto the chatbot’s responses.\n\nAria works in much the same way as ChatGPT, Bing and other chatbots — it accepts natural-language queries and provides plain-English results within a second or two. Like other chatbots, it’s apparently able to generate snippets of code for major programming languages like C++, Java, JS, and Python, as well as its own algorithms for searching and sorting data. It’s even able to create machine learning code through libraries like TensorFlow and scikit-learn.\n\nOpera made a point of noting that Aria was not enabled by default and that users will need to activate it to make use of its features.\n\n“As AI arrives on Opera for iOS, it’s essential to emphasize that the AI experience within Opera is entirely user-controlled,” the company said. “You have the autonomy to opt in based on your preference for engaging with AI services.”\n\nOpera on iOS also includes integration with Apple Intelligent Tracking Prevention, which helps cut down on third-party tracking cookies, as well as a built-in VPN service for more secure browsing.\n\nThe Opera browser, which has been around since 1995, became a Chromium-based application in 2013, and led the way in terms of features that most major browsers now include as standard, including built-in pop-up blockers and tabbed browsing. While it’s not the first to offer an AI assistant —Microsoft, for instance, includes the Bing AI chatbot in Edge — it’s one of the first to integrate the technology across all of the major platforms.\n\nAn Aria-enabled version of Opera is available for iOS now. It’s free to use, although the AI features do require an Opera account.",
    "comments": [
      { "author": "Bob", "date": "8/16/2023, 11:34:00 AM", "text": "I don't use opera." },
      { "author": "Alex", "date": "8/16/2023, 11:59:00 AM", "text": "But some people like opera." }
    ]
  },
  {
    "title": "IT leaders want AI but but accelerating development stirs fears: Report",
    "date": "8/16/2023, 9:30:00 AM",
    "text": "While IT leaders are optimistic about AI, the speed at which the technology is developing has led them to question their organizations' readiness to implement it, according to a report from AMD.\n\nNew research from AMD has found that IT leaders are optimistic about the potential benefits of artificial intelligence, but are worried that their organizations are not prepared to implement the technology, whose development has accelerated with the rapid advancement of generative AI.\n\nThe survey comprised responses from 2,500 IT leaders across the US, UK, Germany, France, and Japan, and sought to better understand the impact AI is having on the workplace.\n\n\"In the past six months alone there has been a marked advancement in AI tech. Wide availability of new, advanced generative AI tools has launched the technology into the mainstream, prompting many organizations to consider where AI fits in their existing digital transformation roadmaps and introducing a range of new concerns,\" according to the report.\n\nMore than two-thirds of the IT leaders polled are now amassing budgets for AI project implementation, with the most frequently cited benefits including automated cybersecurity detection capabilities (70%), improving the efficiency of work models (68%), and an overall increase in employee productivity (67%), the report said.\n\nFurthermore, a majority of those surveyed expressed optimism about AI’s ability to improve their day-to-day work, with 78% believing AI will allow them to accomplish more tasks and 70% saying it will improve their work-life balance.\n\nAI outpaces company preparedness to deal with it\nHowever, the survey also showed that IT leaders think that AI development is outpacing organizational preparedness, causing them to express concern about their implementation roadmap and overall readiness.\n\nAlthough 97% of IT leaders polled said they were familiar with AI, 52% of those surveyed said they had not experimented with the latest natural language processing applications, while 47% and 36% said the same for facial recognition systems and process automation software, respectively.\n\nForty-six percent of respondents said their organization isn’t ready to implement AI, with just 19% stating their company will prioritize AI within the next year. Forty-four percent of IT leaders surveyed said that AI would become a priority for their organization in between one and five years' time.\n\nUnsurprisingly, those leaders who said their organization would be prioritizing AI implementation this year were the most optimistic about the potential benefits of AI, with 75% of those expressing a positive view, believing that not investing would be a risk, as it could result in their organization falling behind their industry competitors.\n\n“There is a benefit to being an early AI adopter,” said Matthew Unangst, senior director, commercial client and workstation at AMD, in comments posted alongside the research. “IT leaders are seeing the benefits of AI-enabled solutions, but their enterprises need to outline a more focused plan for implementation or risk falling behind.”",
    "comments": [
      { "author": "Kelly", "date": "8/16/2023, 10:03:00 AM", "text": "Artificial intelligence is very promising." },
      { "author": "Bob", "date": "8/16/2023, 11:36:00 AM", "text": "Now this market is developing very fast." }
    ]
  },
  {
    "title": "Intel’s Tower Semiconductor acquisition fails, as China witholds OK",
    "date": "8/16/2023, 8:46:00 AM",
    "text": "Intel's planned $5.4 billion acquisition of Israel-based Tower falls apart, as China fails to approve the deal before the companies' agreement deadline.\n\nIntel's planned $5.4 billion acquisition of Israel-based Tower Semiconductor has fallen apart, as China reportedly failed to approve the deal in time to meet a deadline agreed upon by the two companies for the deal to close.\n\nIntel said in a statement late Tuesday said that the two companies mutually agreed to terminate the deal \"due to the inability to obtain in a timely manner the regulatory approvals required under the merger agreement.\" The company added that it will pay an agreed-upon termination fee of $353 million to Tower.\n\nThe failed deal can be seen as a victim of the ongoing tech trade war between the US and China. Neither Intel nor Tower identified China as the regulatory holdout. But both companies have facilities in the country, which retains the right to approve mergers and acquisitions of companies that generate revenue from affiliates within its borders, and multiple media reports note that Intel has let it be understood that regulatory approval was extended by authorities except for Chinese regulators.\n\nThe deal, which was first announced in February 2022, saw its timeline extended at least two times before Intel decided to walk away from it.\n\nThe Tower acquisition, according to Intel, was part of its Integrated Device Manufacturing (IDM) 2.0 strategy, aimed at garnering more share of the foundry services market.\n\nIntel had planned to take advantage of Tower’s expertise in radio frequency (RF), power, silicon-germanium (SiGe) and industrial sensor technologies, as well as its extensive IP and electronic design automation (EDA) partnerships.\n\nThe acquisition would also have provided Intel access to Tower’s established foundry footprint across high-growth markets such as mobile, electric vehicles, and power.\n\nHowever, Intel CEO Pat Gelsinger said that the termination of the deal wouldn’t affect the company’s IDM 2.0 roadmap and strategy.\n\n“We are executing well on our roadmap to regain transistor performance and power performance leadership by 2025, building momentum with customers and the broader ecosystem and investing to deliver the geographically diverse and resilient manufacturing footprint the world needs,” Gelsinger said in the statement.\n\nDeal fails amid US-China trade war\nThe failed deal comes amidst the escalating trade war between US and China, which under US President Joe Biden has focused on the semiconductor sector. Just last week, US President Joe Biden ratcheted up the technology trade war by issuing an executive order that will restrict investment in several sectors in China, including semiconductors and AI.\n\nBiden’s order comes after a series of moves by the US to restrict China’s access to advanced chips.\n\nThe US first imposed restrictions on exports of chips to China in 2015, extending them in 2021 and twice in 2022. The most recent restrictions were introduced in December last year.\n\nUS lawmakers have also been urging the Biden administration to take more action to impede China’s progress in gaining dominance in areas such as artificial intelligence and quantum computing.\n\nIn January, the US convinced the Netherlands and Japan to join it in expanding the ban on exports of chip-making technology to China.\n\nAccording to analysts, Washington’s strategy to strike a deal with the two countries was a significant move, as some of the world’s largest manufacturers of semiconductor manufacturing equipment are headquartered in these nations.\n\nThe deal was followed by Beijing's ban on the use of semiconductors manufactured by US-based chipmaker Micron, followed by Japanese restrictions on chip exports to China.\n\nChina has urged Japan to repeal the restrictions, citing international and trade regulations violations.\n\nMeanwhile, Beijing has imposed restrictions on the import of gallium and germanium — two metals needed for manufacturing semiconductors.",
    "comments": [
      { "author": "Sam", "date": "8/16/2023, 8:49:00 AM", "text": "They should not listen to China." },
      { "author": "Jim", "date": "8/16/2023, 8:51:00 AM", "text": "Totally agree with Sam." }
    ]
  }
]
